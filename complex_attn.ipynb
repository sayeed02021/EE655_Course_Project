{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828bc82",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 9.935779,
     "end_time": "2025-04-18T14:46:23.722647",
     "exception": false,
     "start_time": "2025-04-18T14:46:13.786868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece50118",
   "metadata": {
    "papermill": {
     "duration": 0.003699,
     "end_time": "2025-04-18T14:46:23.730626",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.726927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complex Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4680b",
   "metadata": {
    "papermill": {
     "duration": 0.030861,
     "end_time": "2025-04-18T14:46:23.765312",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.734451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_complex(fr, fi, input, dtype= torch.complex64):\n",
    "    return (fr(input.real) - fi(input.imag)).type(dtype) + 1j * (fr(input.imag) + fi(input.real)).type(dtype)\n",
    "\n",
    "\n",
    "\n",
    "class ComplexConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        bias: bool = False,\n",
    "        complex_axis= 1,\n",
    "        device= None,\n",
    "        dtype= None\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_real = nn.Conv2d(in_channels, out_channels, kernel_size= kernel_size, stride= stride, padding= padding,  bias= bias)\n",
    "        self.conv_imag = nn.Conv2d(in_channels, out_channels, kernel_size= kernel_size, stride= stride, padding= padding,  bias= bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' define how the forward prop will take place '''\n",
    "        # check if the input is of dtype complex\n",
    "        # for this we can use is_complex() function which will return true if the input is complex dtype\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor. Got {x.dtype}\")\n",
    "\n",
    "        return apply_complex(self.conv_real, self.conv_imag, x)\n",
    "    \n",
    "\n",
    "class ComplexTranspose2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        output_padding=0,\n",
    "        bias: bool= False,\n",
    "        device= None,\n",
    "        dtype= None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans_conv_real = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride= stride, padding= padding, output_padding= output_padding,  bias= bias)\n",
    "        self.trans_conv_imag = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride= stride, padding= padding, output_padding= output_padding,  bias= bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' define how the forward prop will take place '''\n",
    "        # check if the input is of dtype complex\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor. Got {x.dtype}\")\n",
    "\n",
    "        return apply_complex(self.trans_conv_real, self.trans_conv_imag, x)\n",
    "    \n",
    "\n",
    "class ComplexMaxPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, stride= 2, padding= 0, dilation=(1,1), return_indices= False, ceil_mode= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size= kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding= padding\n",
    "        self.dilation = dilation\n",
    "        self.ceil_mode= ceil_mode\n",
    "        self.return_indices= return_indices\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(self.kernel_size, self.stride, self.padding, self.dilation, \n",
    "                                     self.return_indices, self.ceil_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # check if the input is complex\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor, Got {x.dtype}\")\n",
    "\n",
    "        return (self.max_pool(x.real)).type(torch.complex64) + 1j * (self.max_pool(x.imag)).type(torch.complex64)\n",
    "    \n",
    "\n",
    "class CReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x.real).type(torch.complex64) + 1j * F.relu(x.imag).type(torch.complex64)\n",
    "    \n",
    "\n",
    "\n",
    "class Naive_ComplexSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(x.real).type(torch.complex64) + 1j * F.sigmoid(x.imag).type(torch.complex64)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ComplexBatchNorm2d(torch.nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
    "            track_running_stats=True, complex_axis=1):\n",
    "        super().__init__()\n",
    "        self.num_features        = num_features\n",
    "        self.eps                 = eps\n",
    "        self.momentum            = momentum\n",
    "        self.affine              = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "        if self.affine:\n",
    "            self.Wrr = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wri = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wii = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Br  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Bi  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "        else:\n",
    "            self.register_parameter('Wrr', None)\n",
    "            self.register_parameter('Wri', None)\n",
    "            self.register_parameter('Wii', None)\n",
    "            self.register_parameter('Br',  None)\n",
    "            self.register_parameter('Bi',  None)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('RMr',  torch.zeros(self.num_features))\n",
    "            self.register_buffer('RMi',  torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVrr', torch.ones (self.num_features))\n",
    "            self.register_buffer('RVri', torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVii', torch.ones (self.num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_parameter('RMr',                 None)\n",
    "            self.register_parameter('RMi',                 None)\n",
    "            self.register_parameter('RVrr',                None)\n",
    "            self.register_parameter('RVri',                None)\n",
    "            self.register_parameter('RVii',                None)\n",
    "            self.register_parameter('num_batches_tracked', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self):\n",
    "        if self.track_running_stats:\n",
    "            self.RMr.zero_()\n",
    "            self.RMi.zero_()\n",
    "            self.RVrr.fill_(1)\n",
    "            self.RVri.zero_()\n",
    "            self.RVii.fill_(1)\n",
    "            self.num_batches_tracked.zero_()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            self.Br.data.zero_()\n",
    "            self.Bi.data.zero_()\n",
    "            self.Wrr.data.fill_(1)\n",
    "            self.Wri.data.uniform_(-.9, +.9) # W will be positive-definite\n",
    "            self.Wii.data.fill_(1)\n",
    "\n",
    "    def _check_input_dim(self, xr, xi):\n",
    "        assert(xr.shape == xi.shape)\n",
    "        assert(xr.size(1) == self.num_features)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #self._check_input_dim(xr, xi)\n",
    "\n",
    "        # xr, xi = torch.chunk(inputs,2, axis=self.complex_axis)\n",
    "        xr, xi = inputs.real, inputs.imag\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "            if self.momentum is None:  # use cumulative moving average\n",
    "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
    "            else:  # use exponential moving average\n",
    "                exponential_average_factor = self.momentum\n",
    "\n",
    "        #\n",
    "        # NOTE: The precise meaning of the \"training flag\" is:\n",
    "        #       True:  Normalize using batch   statistics, update running statistics\n",
    "        #              if they are being collected.\n",
    "        #       False: Normalize using running statistics, ignore batch   statistics.\n",
    "        #\n",
    "        training = self.training or not self.track_running_stats\n",
    "        redux = [i for i in reversed(range(xr.dim())) if i!=1]\n",
    "        vdim  = [1] * xr.dim()\n",
    "        vdim[1] = xr.size(1)\n",
    "\n",
    "        #\n",
    "        # Mean M Computation and Centering\n",
    "        #\n",
    "        # Includes running mean update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Mr, Mi = xr, xi\n",
    "            for d in redux:\n",
    "                Mr = Mr.mean(d, keepdim=True)\n",
    "                Mi = Mi.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RMr.lerp_(Mr.squeeze(), exponential_average_factor)\n",
    "                self.RMi.lerp_(Mi.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Mr = self.RMr.view(vdim)\n",
    "            Mi = self.RMi.view(vdim)\n",
    "        xr, xi = xr-Mr, xi-Mi\n",
    "\n",
    "        #\n",
    "        # Variance Matrix V Computation\n",
    "        #\n",
    "        # Includes epsilon numerical stabilizer/Tikhonov regularizer.\n",
    "        # Includes running variance update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Vrr = xr * xr\n",
    "            Vri = xr * xi\n",
    "            Vii = xi * xi\n",
    "            for d in redux:\n",
    "                Vrr = Vrr.mean(d, keepdim=True)\n",
    "                Vri = Vri.mean(d, keepdim=True)\n",
    "                Vii = Vii.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RVrr.lerp_(Vrr.squeeze(), exponential_average_factor)\n",
    "                self.RVri.lerp_(Vri.squeeze(), exponential_average_factor)\n",
    "                self.RVii.lerp_(Vii.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Vrr = self.RVrr.view(vdim)\n",
    "            Vri = self.RVri.view(vdim)\n",
    "            Vii = self.RVii.view(vdim)\n",
    "        Vrr   = Vrr + self.eps\n",
    "        Vri   = Vri\n",
    "        Vii   = Vii + self.eps\n",
    "\n",
    "        #\n",
    "        # Matrix Inverse Square Root U = V^-0.5\n",
    "        #\n",
    "        # sqrt of a 2x2 matrix,\n",
    "        # - https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
    "        tau   = Vrr + Vii\n",
    "        # delta = torch.addcmul(Vrr * Vii, -1, Vri, Vri)\n",
    "        delta = torch.addcmul(Vrr * Vii, Vri, Vri, value= -1)\n",
    "        s     = delta.sqrt()\n",
    "        t     = (tau + 2*s).sqrt()\n",
    "\n",
    "        # matrix inverse, http://mathworld.wolfram.com/MatrixInverse.html\n",
    "        rst   = (s * t).reciprocal()\n",
    "        Urr   = (s + Vii) * rst\n",
    "        Uii   = (s + Vrr) * rst\n",
    "        Uri   = (  - Vri) * rst\n",
    "\n",
    "        #\n",
    "        # Optionally left-multiply U by affine weights W to produce combined\n",
    "        # weights Z, left-multiply the inputs by Z, then optionally bias them.\n",
    "        #\n",
    "        # y = Zx + B\n",
    "        # y = WUx + B\n",
    "        # y = [Wrr Wri][Urr Uri] [xr] + [Br]\n",
    "        #     [Wir Wii][Uir Uii] [xi]   [Bi]\n",
    "        #\n",
    "        if self.affine:\n",
    "            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)\n",
    "            Zrr = (Wrr * Urr) + (Wri * Uri)\n",
    "            Zri = (Wrr * Uri) + (Wri * Uii)\n",
    "            Zir = (Wri * Urr) + (Wii * Uri)\n",
    "            Zii = (Wri * Uri) + (Wii * Uii)\n",
    "        else:\n",
    "            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii\n",
    "\n",
    "        yr = (Zrr * xr) + (Zri * xi)\n",
    "        yi = (Zir * xr) + (Zii * xi)\n",
    "\n",
    "        if self.affine:\n",
    "            yr = yr + self.Br.view(vdim)\n",
    "            yi = yi + self.Bi.view(vdim)\n",
    "\n",
    "        return (yr).type(torch.complex64) + 1j * (yi).type(torch.complex64)\n",
    "\n",
    "# Add to complex_utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ComplexChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # Use standard Conv2d for processing magnitude (real) values\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel//reduction_ratio, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel//reduction_ratio, channel, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract magnitude\n",
    "        mag = torch.sqrt(x.real**2 + x.imag**2 + 1e-8)\n",
    "        \n",
    "        # Process with real-valued operations\n",
    "        avg_pool = self.avg_pool(mag)\n",
    "        max_pool = self.max_pool(mag)\n",
    "        avg_out = self.fc(avg_pool)\n",
    "        max_out = self.fc(max_pool)\n",
    "        \n",
    "        # Generate attention weights\n",
    "        scale = torch.sigmoid(avg_out + max_out)\n",
    "        \n",
    "        # Apply to both real and imaginary parts\n",
    "        return torch.complex(scale * x.real, scale * x.imag)\n",
    "\n",
    "class ComplexSpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        # Use standard Conv2d for spatial processing\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract magnitude\n",
    "        mag = torch.sqrt(x.real**2 + x.imag**2 + 1e-8)\n",
    "        \n",
    "        # Generate spatial attention maps\n",
    "        avg_out = torch.mean(mag, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(mag, dim=1, keepdim=True)\n",
    "        \n",
    "        # Process with real-valued operations\n",
    "        spatial = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention = torch.sigmoid(self.conv(spatial))\n",
    "        \n",
    "        # Apply to both real and imaginary parts\n",
    "        return torch.complex(attention * x.real, attention * x.imag)\n",
    "\n",
    "class ComplexCBAM(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.channel_att = ComplexChannelAttention(channel)\n",
    "        self.spatial_att = ComplexSpatialAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be99f8",
   "metadata": {
    "papermill": {
     "duration": 0.003541,
     "end_time": "2025-04-18T14:46:23.772633",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.769092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complex UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129627d",
   "metadata": {
    "papermill": {
     "duration": 0.012999,
     "end_time": "2025-04-18T14:46:23.789228",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.776229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_attention=False):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            ComplexConv2d(in_channels, out_channels, kernel_size=3, bias=False),\n",
    "            ComplexBatchNorm2d(out_channels),\n",
    "            CReLU(),\n",
    "            ComplexConv2d(out_channels, out_channels, kernel_size=3, bias=False),\n",
    "            ComplexBatchNorm2d(out_channels),\n",
    "            CReLU()\n",
    "        )\n",
    "        self.attention = ComplexCBAM(out_channels) if use_attention else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.attention(x)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512], \n",
    "                 use_attention=[True, True, True, True]):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = ComplexMaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part with optional attention\n",
    "        for i, feature in enumerate(features):\n",
    "            use_attn = use_attention[i] if i < len(use_attention) else False\n",
    "            self.downs.append(DoubleConv(in_channels, feature, use_attention=use_attn))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part with optional attention in decoder blocks\n",
    "        for i, feature in enumerate(reversed(features)):\n",
    "            self.ups.append(\n",
    "                ComplexTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            use_attn = use_attention[len(features)-i-1] if i < len(use_attention) else False\n",
    "            self.ups.append(DoubleConv(feature*2, feature, use_attention=use_attn))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2, use_attention=True)\n",
    "        self.final_conv = ComplexConv2d(features[0], out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Encoder path\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]  # reverse for decoder path\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            # Concatenate skip connection with upsampled features\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        # Final convolution\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba112e0",
   "metadata": {
    "papermill": {
     "duration": 0.003596,
     "end_time": "2025-04-18T14:46:23.829627",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.826031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddd671",
   "metadata": {
    "papermill": {
     "duration": 0.013557,
     "end_time": "2025-04-18T14:46:23.846840",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.833283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, dir, transforms=None,\n",
    "                 class_dict='CamVid/class_dict.csv', \n",
    "                 mode='train'):\n",
    "        super().__init__()\n",
    "        self.image_dir = f'{dir}/{mode}'\n",
    "        self.image_paths = natsorted(glob.glob(self.image_dir+'/*.png'))\n",
    "        self.mask_dir = f'{dir}/{mode}_labels'\n",
    "        self.mask_paths = natsorted(glob.glob(self.mask_dir+'/*.png'))\n",
    "        self.color_map, self.color_to_idx = self.load_colormap(class_dict)\n",
    "        if transforms is not None:\n",
    "            self.img_transforms = transforms[0]\n",
    "            self.mask_transforms = transforms[1]\n",
    "        else:\n",
    "            self.img_transforms = None\n",
    "            self.mask_transforms = None\n",
    "\n",
    "    def load_colormap(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        color_map = [tuple(row) for row in df[['r', 'g', 'b']].values]\n",
    "        color_to_idx = {color:idx for idx, color in enumerate(color_map)}\n",
    "        return color_map, color_to_idx\n",
    "    \n",
    "    def encode_segmap(self, mask):\n",
    "        mask = np.array(mask)\n",
    "        mask_encoded = np.zeros(mask.shape[:2], dtype=np.int64)\n",
    "        for color, idx in self.color_to_idx.items():\n",
    "            mask_encoded[np.all(mask==color, axis=-1)]=idx\n",
    "\n",
    "        return torch.tensor(mask_encoded, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        mask_path = self.mask_paths[index]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        if self.img_transforms is not None:\n",
    "            image = self.img_transforms(image)\n",
    "        if self.mask_transforms is not None:\n",
    "            mask = self.mask_transforms(mask)\n",
    "        \n",
    "        mask = self.encode_segmap(mask)\n",
    "        return image, mask, img_path\n",
    "    \n",
    "\n",
    "class CamVid_Simple(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = glob.glob(path+'/*.pth')\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.load(self.files[index])\n",
    "        return data['img'], data['mask']   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb696219",
   "metadata": {
    "papermill": {
     "duration": 0.003496,
     "end_time": "2025-04-18T14:46:23.854042",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.850546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88472863",
   "metadata": {
    "papermill": {
     "duration": 0.009351,
     "end_time": "2025-04-18T14:46:23.867163",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.857812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Complex_CCELoss(nn.Module):\n",
    "    def __init__(self, lambda_phase=0.2):\n",
    "        super(Complex_CCELoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lambda_phase = lambda_phase\n",
    "\n",
    "    def forward(self, z, target):\n",
    "        \n",
    "\n",
    "        l_real = self.criterion(torch.abs(z), target)\n",
    "\n",
    "        phase_select = torch.gather(torch.angle(z), dim=1, index = target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        l_phase = (1 - torch.cos(phase_select)).mean()  # More stable\n",
    "\n",
    "        return l_real + self.lambda_phase * l_phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e30c14",
   "metadata": {
    "papermill": {
     "duration": 0.003559,
     "end_time": "2025-04-18T14:46:23.874483",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.870924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25eb7c4",
   "metadata": {
    "papermill": {
     "duration": 0.003458,
     "end_time": "2025-04-18T14:46:23.881504",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.878046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train 1st loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44222eaf",
   "metadata": {
    "papermill": {
     "duration": 0.00793,
     "end_time": "2025-04-18T14:46:23.893139",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.885209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c55f5b",
   "metadata": {
    "papermill": {
     "duration": 0.008444,
     "end_time": "2025-04-18T14:46:23.905319",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.896875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 32\n",
    "batch_size = 8\n",
    "num_epochs = 25\n",
    "lr = 1e-3\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "complex_model  = UNET(in_channels = 3, out_channels=num_classes)\n",
    "complex_model = complex_model.to(device)\n",
    "complex_model.train()\n",
    "\n",
    "traindataset = CamVid_Simple(path = 'path to train dataset')\n",
    "\n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, num_epochs*len(trainloader), eta_min=1e-5)\n",
    "\n",
    "criterion = Complex_CCELoss()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_avg = 0\n",
    "    pbar = tqdm(trainloader)\n",
    "    for batch_idx, (image, mask) in enumerate(pbar):\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        output = complex_model(image)\n",
    "        loss = criterion(output, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_avg+=loss.item()\n",
    "\n",
    "        descrip = {\n",
    "            'Epoch': epoch+1,\n",
    "            'Loss': loss_avg/(batch_idx+1),\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        pbar.set_postfix(descrip)\n",
    "    pbar.close()\n",
    "    torch.save(complex_model.state_dict(), f'complex_model_l1.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea0fa8",
   "metadata": {
    "papermill": {
     "duration": 0.003428,
     "end_time": "2025-04-18T14:46:23.912891",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.909463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate for first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2de519",
   "metadata": {
    "papermill": {
     "duration": 0.008455,
     "end_time": "2025-04-18T14:46:23.925009",
     "exception": false,
     "start_time": "2025-04-18T14:46:23.916554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import JaccardIndex\n",
    "testdataset = CamVid_Simple(path = 'path to test dataset')\n",
    "device = 'cuda'\n",
    "complex_model.to(device)\n",
    "num_classes = 32\n",
    "batch_size = 8\n",
    "torch.cuda.empty_cache()\n",
    "metric = JaccardIndex(task='multiclass', num_classes=32)\n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "complex_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx,(img, target) in enumerate(tqdm(testloader)):\n",
    "        img = img.to(device)\n",
    "        output  = complex_model(img)\n",
    "        # output = output.real*output.imag\n",
    "        output = torch.abs(output)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        metric.update(output.cpu(), target)\n",
    "\n",
    "    iou = metric.compute()\n",
    "    print('Jaccard index Score: ', iou)\n",
    "\n",
    "    metric.reset()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7143893,
     "sourceId": 11429472,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 310214,
     "modelInstanceId": 289476,
     "sourceId": 346468,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3525.692247,
   "end_time": "2025-04-18T15:44:55.403400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T14:46:09.711153",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
