{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73a218-f42f-4f8d-86e6-37ed0f9e59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b49dae-85b2-41df-8b84-aa473a1473c2",
   "metadata": {},
   "source": [
    "# Complex Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded22137-b218-420a-86aa-386e88f4af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of the code is taken from the following GitHub Repo: \n",
    "https://github.com/saurabhya/FCCNs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def apply_complex(fr, fi, input, dtype= torch.complex64):\n",
    "    return (fr(input.real) - fi(input.imag)) + 1j * (fr(input.imag) + fi(input.real))\n",
    "\n",
    "\n",
    "\n",
    "class ComplexConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        bias: bool = False,\n",
    "        complex_axis= 1,\n",
    "        device= None,\n",
    "        dtype= None\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_real = nn.Conv2d(in_channels, out_channels, kernel_size= kernel_size, stride= stride, padding= padding,  bias= bias)\n",
    "        self.conv_imag = nn.Conv2d(in_channels, out_channels, kernel_size= kernel_size, stride= stride, padding= padding,  bias= bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' define how the forward prop will take place '''\n",
    "        # check if the input is of dtype complex\n",
    "        # for this we can use is_complex() function which will return true if the input is complex dtype\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor. Got {x.dtype}\")\n",
    "\n",
    "        return apply_complex(self.conv_real, self.conv_imag, x)\n",
    "    \n",
    "\n",
    "class ComplexTranspose2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        output_padding=0,\n",
    "        bias: bool= False,\n",
    "        device= None,\n",
    "        dtype= None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans_conv_real = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride= stride, padding= padding, output_padding= output_padding,  bias= bias)\n",
    "        self.trans_conv_imag = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride= stride, padding= padding, output_padding= output_padding,  bias= bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' define how the forward prop will take place '''\n",
    "        # check if the input is of dtype complex\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor. Got {x.dtype}\")\n",
    "\n",
    "        return apply_complex(self.trans_conv_real, self.trans_conv_imag, x)\n",
    "    \n",
    "\n",
    "class ComplexMaxPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, stride= 2, padding= 0, dilation=(1,1), return_indices= False, ceil_mode= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size= kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding= padding\n",
    "        self.dilation = dilation\n",
    "        self.ceil_mode= ceil_mode\n",
    "        self.return_indices= return_indices\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(self.kernel_size, self.stride, self.padding, self.dilation, \n",
    "                                     self.return_indices, self.ceil_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # check if the input is complex\n",
    "        if not x.is_complex():\n",
    "            raise ValueError(f\"Input should be a complex tensor, Got {x.dtype}\")\n",
    "\n",
    "        return (self.max_pool(x.real)) + 1j * (self.max_pool(x.imag))\n",
    "    \n",
    "\n",
    "class CReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.real_coeff = nn.Parameter(torch.tensor(1.0))\n",
    "        self.imag_coeff = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # real_part = F.relu(self.real_coeff*F.relu(x.real)) # Uncomment for normal use\n",
    "        # imag_part = F.relu(self.imag_coeff*F.relu(x.imag))\n",
    "        real_part = F.relu(self.real_coeff*x.real)\n",
    "        imag_part = F.relu(self.imag_coeff*x.imag)\n",
    "        return real_part + 1j * imag_part\n",
    "    \n",
    "\n",
    "\n",
    "class Naive_ComplexSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(x.real) + 1j * F.sigmoid(x.imag)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ComplexBatchNorm2d(torch.nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
    "            track_running_stats=True, complex_axis=1):\n",
    "        super().__init__()\n",
    "        self.num_features        = num_features\n",
    "        self.eps                 = eps\n",
    "        self.momentum            = momentum\n",
    "        self.affine              = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "        if self.affine:\n",
    "            self.Wrr = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wri = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wii = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Br  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Bi  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "        else:\n",
    "            self.register_parameter('Wrr', None)\n",
    "            self.register_parameter('Wri', None)\n",
    "            self.register_parameter('Wii', None)\n",
    "            self.register_parameter('Br',  None)\n",
    "            self.register_parameter('Bi',  None)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('RMr',  torch.zeros(self.num_features))\n",
    "            self.register_buffer('RMi',  torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVrr', torch.ones (self.num_features))\n",
    "            self.register_buffer('RVri', torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVii', torch.ones (self.num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_parameter('RMr',                 None)\n",
    "            self.register_parameter('RMi',                 None)\n",
    "            self.register_parameter('RVrr',                None)\n",
    "            self.register_parameter('RVri',                None)\n",
    "            self.register_parameter('RVii',                None)\n",
    "            self.register_parameter('num_batches_tracked', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self):\n",
    "        if self.track_running_stats:\n",
    "            self.RMr.zero_()\n",
    "            self.RMi.zero_()\n",
    "            self.RVrr.fill_(1)\n",
    "            self.RVri.zero_()\n",
    "            self.RVii.fill_(1)\n",
    "            self.num_batches_tracked.zero_()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            self.Br.data.zero_()\n",
    "            self.Bi.data.zero_()\n",
    "            self.Wrr.data.fill_(1)\n",
    "            self.Wri.data.uniform_(-.9, +.9) # W will be positive-definite\n",
    "            self.Wii.data.fill_(1)\n",
    "\n",
    "    def _check_input_dim(self, xr, xi):\n",
    "        assert(xr.shape == xi.shape)\n",
    "        assert(xr.size(1) == self.num_features)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #self._check_input_dim(xr, xi)\n",
    "\n",
    "        # xr, xi = torch.chunk(inputs,2, axis=self.complex_axis)\n",
    "        xr, xi = inputs.real, inputs.imag\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "            if self.momentum is None:  # use cumulative moving average\n",
    "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
    "            else:  # use exponential moving average\n",
    "                exponential_average_factor = self.momentum\n",
    "\n",
    "        #\n",
    "        # NOTE: The precise meaning of the \"training flag\" is:\n",
    "        #       True:  Normalize using batch   statistics, update running statistics\n",
    "        #              if they are being collected.\n",
    "        #       False: Normalize using running statistics, ignore batch   statistics.\n",
    "        #\n",
    "        training = self.training or not self.track_running_stats\n",
    "        redux = [i for i in reversed(range(xr.dim())) if i!=1]\n",
    "        vdim  = [1] * xr.dim()\n",
    "        vdim[1] = xr.size(1)\n",
    "\n",
    "        #\n",
    "        # Mean M Computation and Centering\n",
    "        #\n",
    "        # Includes running mean update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Mr, Mi = xr, xi\n",
    "            for d in redux:\n",
    "                Mr = Mr.mean(d, keepdim=True)\n",
    "                Mi = Mi.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RMr.lerp_(Mr.squeeze(), exponential_average_factor)\n",
    "                self.RMi.lerp_(Mi.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Mr = self.RMr.view(vdim)\n",
    "            Mi = self.RMi.view(vdim)\n",
    "        xr, xi = xr-Mr, xi-Mi\n",
    "\n",
    "        #\n",
    "        # Variance Matrix V Computation\n",
    "        #\n",
    "        # Includes epsilon numerical stabilizer/Tikhonov regularizer.\n",
    "        # Includes running variance update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Vrr = xr * xr\n",
    "            Vri = xr * xi\n",
    "            Vii = xi * xi\n",
    "            for d in redux:\n",
    "                Vrr = Vrr.mean(d, keepdim=True)\n",
    "                Vri = Vri.mean(d, keepdim=True)\n",
    "                Vii = Vii.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RVrr.lerp_(Vrr.squeeze(), exponential_average_factor)\n",
    "                self.RVri.lerp_(Vri.squeeze(), exponential_average_factor)\n",
    "                self.RVii.lerp_(Vii.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Vrr = self.RVrr.view(vdim)\n",
    "            Vri = self.RVri.view(vdim)\n",
    "            Vii = self.RVii.view(vdim)\n",
    "        Vrr   = Vrr + self.eps\n",
    "        Vri   = Vri\n",
    "        Vii   = Vii + self.eps\n",
    "\n",
    "        #\n",
    "        # Matrix Inverse Square Root U = V^-0.5\n",
    "        #\n",
    "        # sqrt of a 2x2 matrix,\n",
    "        # - https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
    "        tau   = Vrr + Vii\n",
    "        # delta = torch.addcmul(Vrr * Vii, -1, Vri, Vri)\n",
    "        delta = torch.addcmul(Vrr * Vii, Vri, Vri, value= -1)\n",
    "        s     = delta.sqrt()\n",
    "        t     = (tau + 2*s).sqrt()\n",
    "\n",
    "        # matrix inverse, http://mathworld.wolfram.com/MatrixInverse.html\n",
    "        rst   = (s * t).reciprocal()\n",
    "        Urr   = (s + Vii) * rst\n",
    "        Uii   = (s + Vrr) * rst\n",
    "        Uri   = (  - Vri) * rst\n",
    "\n",
    "        #\n",
    "        # Optionally left-multiply U by affine weights W to produce combined\n",
    "        # weights Z, left-multiply the inputs by Z, then optionally bias them.\n",
    "        #\n",
    "        # y = Zx + B\n",
    "        # y = WUx + B\n",
    "        # y = [Wrr Wri][Urr Uri] [xr] + [Br]\n",
    "        #     [Wir Wii][Uir Uii] [xi]   [Bi]\n",
    "        #\n",
    "        if self.affine:\n",
    "            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)\n",
    "            Zrr = (Wrr * Urr) + (Wri * Uri)\n",
    "            Zri = (Wrr * Uri) + (Wri * Uii)\n",
    "            Zir = (Wri * Urr) + (Wii * Uri)\n",
    "            Zii = (Wri * Uri) + (Wii * Uii)\n",
    "        else:\n",
    "            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii\n",
    "\n",
    "        yr = (Zrr * xr) + (Zri * xi)\n",
    "        yi = (Zir * xr) + (Zii * xi)\n",
    "\n",
    "        if self.affine:\n",
    "            yr = yr + self.Br.view(vdim)\n",
    "            yi = yi + self.Bi.view(vdim)\n",
    "\n",
    "        return (yr) + 1j * (yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c8e1a-ed4b-4fa3-8736-68d4accfa83e",
   "metadata": {},
   "source": [
    "# Complex UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef714161-1a8e-4c7a-a701-b750d340e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implemented architecture following: https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            ComplexConv2d(in_channels, out_channels, kernel_size=3, bias=False),\n",
    "            ComplexBatchNorm2d(out_channels),\n",
    "            CReLU(),\n",
    "            ComplexConv2d(out_channels, out_channels, kernel_size=3, bias=False),\n",
    "            ComplexBatchNorm2d(out_channels),\n",
    "            CReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups=nn.ModuleList()\n",
    "        self.downs=nn.ModuleList()\n",
    "        self.pool=ComplexMaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels=feature\n",
    "\n",
    "        \n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                ComplexTranspose2d(\n",
    "                feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "\n",
    "        self.bottleneck=DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv=ComplexConv2d(features[0], out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x=down(x)\n",
    "            skip_connections.append(x)\n",
    "            x=self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1] # reverses order\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x=self.ups[idx](x)\n",
    "            skip_connection=skip_connections[idx//2]\n",
    "\n",
    "            # if x.shape != skip_connection.shape:\n",
    "            #     x=TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip=torch.cat((skip_connection, x), dim=1)\n",
    "            x=self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac5ad0-c8c3-4708-88c0-d88f6824470b",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc9e39-154d-4a3a-a8c3-e0ecd1f87ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamVid_Simple(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = glob.glob(path+'/*.pth')\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.load(self.files[index])\n",
    "        return data['img'], data['mask']   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d74c13-d879-49c9-acfd-689edfd46992",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f42304-db9d-462a-9537-53062b9b367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch implementation for first loss in the write up\n",
    "\n",
    "\"\"\"\n",
    "class Complex_CCELoss(nn.Module):\n",
    "    def __init__(self, lambda_phase=0.2):\n",
    "        super(Complex_CCELoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lambda_phase = lambda_phase\n",
    "\n",
    "    def forward(self, z, target):\n",
    "        \n",
    "\n",
    "        l_real = self.criterion(torch.abs(z), target)\n",
    "\n",
    "        phase_select = torch.gather(torch.angle(z), dim=1, index = target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        l_phase = (1 - torch.cos(phase_select)).mean()  \n",
    "\n",
    "        return l_real + self.lambda_phase * l_phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9b780-1342-4164-89fe-ad408a2f0f43",
   "metadata": {},
   "source": [
    "# Train Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f053f9a-7a5a-4321-be14-c9d34513b3b5",
   "metadata": {},
   "source": [
    "## Using CE + Phase Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814a6fd-a36e-400b-a268-9466ee2ef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 32\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "lr = 1e-3\n",
    "fpath = 'Dataset/Complex_CamVid_iHSV' # change if required\n",
    "os.makedirs('Models', exist_ok=True)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model  = UNET(in_channels = 3, out_channels=num_classes)\n",
    "model  = model.to(device)\n",
    "model.train()\n",
    "\n",
    "traindataset = CamVid_Simple(path = f'{fpath}/train')\n",
    "\n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, num_epochs*len(trainloader), eta_min=1e-5)\n",
    "\n",
    "criterion = Complex_CCELoss()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_avg = 0\n",
    "    pbar = tqdm(trainloader)\n",
    "    for batch_idx, (image, mask) in enumerate(pbar):\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_avg+=loss.item()\n",
    "\n",
    "        descrip = {\n",
    "            'Epoch': epoch+1,\n",
    "            'Loss': loss_avg/(batch_idx+1),\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        pbar.set_postfix(descrip)\n",
    "    pbar.close()\n",
    "    torch.save(model.state_dict(), f'complex_model_l1.pth') # Change name accordingly\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6ff4-ea77-42d7-ae8c-b900514e6448",
   "metadata": {},
   "source": [
    "### Evaluate First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fbfe14-808d-49fb-bd73-c3e778976236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import JaccardIndex\n",
    "testdataset = CamVid_Simple(path = f'{fpath}/test')\n",
    "num_classes = 32\n",
    "batch_size = 8\n",
    "torch.cuda.empty_cache()\n",
    "metric = JaccardIndex(task='multiclass', num_classes=32)\n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "complex_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx,(img, target) in enumerate(tqdm(testloader)):\n",
    "        img = img.to(device)\n",
    "        output  = complex_model(img)\n",
    "        # output = output.real*output.imag\n",
    "        output = torch.abs(output)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        metric.update(output.cpu(), target)\n",
    "\n",
    "    iou = metric.compute()\n",
    "    print('Jaccard index Score: ', iou)\n",
    "\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec31c8-947e-4c27-93f7-c09c743e0eec",
   "metadata": {},
   "source": [
    "## Using CE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfca338-bed8-40c6-afe5-3485728ab2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 32\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "lr = 1e-3\n",
    "device = torch.device('cuda')\n",
    "\n",
    "fpath = 'Dataset/Complex_CamVid_iHSV' # change if required\n",
    "os.makedirs('Models', exist_ok=True)\n",
    "\n",
    "model  = UNET(in_channels = 3, out_channels=num_classes)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "traindataset = CamVid_Simple(path = f'{fpath}/train')\n",
    "\n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, num_epochs*len(trainloader), eta_min=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_avg = 0\n",
    "    pbar = tqdm(trainloader)\n",
    "    for batch_idx, (image, mask) in enumerate(pbar):\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        output = model(image)\n",
    "        output = (output.real*output.imag).float()\n",
    "        \n",
    "        loss = criterion(output, mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_avg+=loss.item()\n",
    "\n",
    "        descrip = {\n",
    "            'Epoch': epoch+1,\n",
    "            'Loss': loss_avg/(batch_idx+1),\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        pbar.set_postfix(descrip)\n",
    "    pbar.close()\n",
    "    torch.save(model.state_dict(), f'complex_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b3641-ca1e-4dbd-9b41-53606b220da3",
   "metadata": {},
   "source": [
    "### Evaluate 2nd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caac2a-dc98-4a3b-a458-971f4a923dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import JaccardIndex\n",
    "testdataset = CamVid_Simple(path = f'{fpath}/test')\n",
    "num_classes = 32\n",
    "batch_size = 8\n",
    "torch.cuda.empty_cache()\n",
    "metric = JaccardIndex(task='multiclass', num_classes=32)\n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx,(img, target) in enumerate(tqdm(testloader)):\n",
    "        img = img.to(device)\n",
    "        output  = model(img)\n",
    "        output = output.real*output.imag\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        metric.update(output.cpu(), target)\n",
    "\n",
    "    iou = metric.compute()\n",
    "    print('Jaccard index Score: ', iou)\n",
    "\n",
    "    metric.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
